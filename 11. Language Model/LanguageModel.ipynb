{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LanguageModel.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAJldAyzepOb"
      },
      "source": [
        "### **INITIALIZATION:**\n",
        "- I use these three lines of code on top of my each notebooks because it will help to prevent any problems while reloading the same project. And the third line of code helps to make visualization within the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VO53Px9xdrmE"
      },
      "source": [
        "#@ INITIALIZATION: \n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk8Co18Leyiy"
      },
      "source": [
        "**LIBRARIES AND DEPENDENCIES:**\n",
        "- I have downloaded all the libraries and dependencies required for the project in one particular cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0I-gBJ1ewPX"
      },
      "source": [
        "#@ INSTALLING DEPENDENCIES: UNCOMMENT BELOW: \n",
        "# !pip install -Uqq fastbook\n",
        "# import fastbook\n",
        "# fastbook.setup_book()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l9ySLgOe5QN"
      },
      "source": [
        "#@ DOWNLOADING LIBRARIES AND DEPENDENCIES: \n",
        "from fastbook import *                              # Getting all the Libraries. \n",
        "from fastai.callback.fp16 import *\n",
        "from fastai.text.all import *                       # Getting all the Libraries."
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCAIhIx-fhYh"
      },
      "source": [
        "### **GETTING THE DATA:**\n",
        "- I will use **Human Numbers** dataset here. It contains the first 10000 numbers written out in English. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcPI-9djfVt2",
        "outputId": "653ba019-8105-4bc7-961a-4583da1edd88"
      },
      "source": [
        "#@ GETTING THE DATASET: \n",
        "path = untar_data(URLs.HUMAN_NUMBERS)               # Path to the Dataset. \n",
        "path.ls()                                           # Inspecting the Dataset. "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#2) [Path('/root/.fastai/data/human_numbers/train.txt'),Path('/root/.fastai/data/human_numbers/valid.txt')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxZ4-F9EgOpz",
        "outputId": "279e473f-8d7f-4147-bdcb-ddc84f2a0039"
      },
      "source": [
        "#@ JOINING AND INSPECTING THE DATASET: \n",
        "lines = L()                                         # Initializing a List. \n",
        "with open(path/\"train.txt\") as f:                   # Opening the File. \n",
        "    lines += L(*f.readlines())                      # Reading the Lines. \n",
        "with open(path/\"valid.txt\") as f:                   # Opening the File. \n",
        "    lines += L(*f.readlines())                      # Reading the Lines. \n",
        "lines                                               # Inspection. "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#9998) ['one \\n','two \\n','three \\n','four \\n','five \\n','six \\n','seven \\n','eight \\n','nine \\n','ten \\n'...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xUisxqHRk16e",
        "outputId": "bd2de8f9-2772-4b4a-d1b5-5a567b0c4190"
      },
      "source": [
        "#@ PREPARING THE DATASET: \n",
        "text = \" . \".join([l.strip() for l in lines])       # Preparing the Dataset. \n",
        "text[:100]                                          # Inspection. "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'one . two . three . four . five . six . seven . eight . nine . ten . eleven . twelve . thirteen . fo'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glIUHjTDl69L"
      },
      "source": [
        "**Note:**\n",
        "- I will tokenize the dataset by splitting on spaces. Then I will create a list of unique tokens called vocab for **Numericalization**. Then I will convert the tokens into numbers by looking up in the index of each in the vocab. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNqUHDHEli9w",
        "outputId": "cf27650c-07b3-49a9-96ee-b6fe03e24d32"
      },
      "source": [
        "#@ TOKENIZING THE DATASET: \n",
        "tokens = text.split(\" \")                            # Splitting into Tokens. \n",
        "tokens[:10]                                         # Inspecting Tokens. "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['one', '.', 'two', '.', 'three', '.', 'four', '.', 'five', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRnH-KHImgIA",
        "outputId": "e91eed6a-e748-43a8-b8cc-ec686d8380a9"
      },
      "source": [
        "#@ GETTING UNIQUE TOKENS: \n",
        "vocab = L(*tokens).unique()                         # Getting Unique Tokens. \n",
        "vocab                                               # Inspection. "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#30) ['one','.','two','three','four','five','six','seven','eight','nine'...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZotFzTfhnCTI",
        "outputId": "e9dc118b-70a2-4f17-f41d-4854372f7a7c"
      },
      "source": [
        "#@ CONVERTING TOKENS INTO NUMBERS: \n",
        "word2idx = {w:i for i,w in enumerate(vocab)}       # Getting Index of Tokens. \n",
        "nums = L(word2idx[i] for i in tokens)              # Converting into Numbers. \n",
        "nums                                               # Inspection.  "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#63095) [0,1,2,1,3,1,4,1,5,1...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_a1MXbJNo0St"
      },
      "source": [
        "### **LANGUAGE MODEL FROM SCRATCH:**\n",
        "- Here I will create a list of every sequence of three words as independent variables and the next word after each sequence as the dependent variable. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7gbp6KZoHpv",
        "outputId": "667edc4e-37f3-4342-cfdf-18894b0206ee"
      },
      "source": [
        "#@ CREATING SEQUENCE OF TOKENS: \n",
        "L((tokens[i:i+3], tokens[i+3]) for i in range(0, len(tokens)-4, 3))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#21031) [(['one', '.', 'two'], '.'),(['.', 'three', '.'], 'four'),(['four', '.', 'five'], '.'),(['.', 'six', '.'], 'seven'),(['seven', '.', 'eight'], '.'),(['.', 'nine', '.'], 'ten'),(['ten', '.', 'eleven'], '.'),(['.', 'twelve', '.'], 'thirteen'),(['thirteen', '.', 'fourteen'], '.'),(['.', 'fifteen', '.'], 'sixteen')...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxKUTFqoqalx",
        "outputId": "e5183290-55e7-40d9-b7e3-5603cf0be0d3"
      },
      "source": [
        "#@ CREATING SEQUENCE OF TENSORS FOR NUMERICALIZED VALUES: \n",
        "seqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0, len(nums)-4, 3))   # Creating Sequence.\n",
        "seqs                                                                           # Inspection.  "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#21031) [(tensor([0, 1, 2]), 1),(tensor([1, 3, 1]), 4),(tensor([4, 1, 5]), 1),(tensor([1, 6, 1]), 7),(tensor([7, 1, 8]), 1),(tensor([1, 9, 1]), 10),(tensor([10,  1, 11]), 1),(tensor([ 1, 12,  1]), 13),(tensor([13,  1, 14]), 1),(tensor([ 1, 15,  1]), 16)...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q--On8YkUWSD"
      },
      "source": [
        "#@ CREATING DATALOADERS: \n",
        "bs = 64                                                 # Initializing Batchsize. \n",
        "cut = int(len(seqs) * 0.8)                              # Initialization. \n",
        "dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], \n",
        "                             bs=bs, shuffle=False)      # Initializing Data Loaders. "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcUWBI0PVpOP"
      },
      "source": [
        "**LANGUAGE MODEL:**\n",
        "- I will create neural network architecture that takes three words as input and returns the predictions of the probability of each possible next word in the vocab. I will use three standard linear layers. The first linear layer will use only the first words embedding as activations. The second layer will use the second words embedding plus the first layers output activations and the third layer will use the third words embedding plus the second layers output activations. The key effect is that every word is interpreted in the information context of any words preceding it. Each of these three layers will use the same weight matrix. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-Djq4aoVoj7"
      },
      "source": [
        "#@ LANGUAGE MODEL IN PYTORCH: SIMPLE LINEAR MODEL: \n",
        "class LMModel1(Module):                                         # Defining Language Model Class. \n",
        "    def __init__(self, vocab_sz, n_hidden):                     # Initializing Constructor Function. \n",
        "        self.i_h = nn.Embedding(vocab_sz, n_hidden)             # Initializing Embedding Layer. \n",
        "        self.h_h = nn.Linear(n_hidden, n_hidden)                # Initializing Linear Layer. \n",
        "        self.h_o = nn.Linear(n_hidden, vocab_sz)                # Initializing Linear Layer. \n",
        "    \n",
        "    def forward(self, x):                                       # Forward Propagation Function. \n",
        "        h = F.relu(self.h_h(self.i_h(x[:, 0])))                 # Implementation of RELU. \n",
        "        h = h + self.i_h(x[:, 1])                               # Second Word Embeddings and Activations. \n",
        "        h = F.relu(self.h_h(h))                                 # Implementation of RELU. \n",
        "        h = h + self.i_h(x[:, 2])                               # Third Word Embeddings and Activations. \n",
        "        h = F.relu(self.h_h(h))                                 # Implementation of RELU. \n",
        "        return self.h_o(h)                                      # Implementation of Linear Layer. "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "cUzzGOm7bAzV",
        "outputId": "5750d48e-0401-4d53-8c69-830db37c4640"
      },
      "source": [
        "#@ TRAINING THE LANGUAGE MODEL: \n",
        "learn = Learner(dls, LMModel1(len(vocab), 64),                  # Initializing Learner with Language Model. \n",
        "                loss_func=F.cross_entropy, metrics=accuracy)    # Initializing Cross Entropy Loss Function. \n",
        "learn.fit_one_cycle(4, 1e-3)                                    # Training the Model. "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.824297</td>\n",
              "      <td>1.970941</td>\n",
              "      <td>0.467554</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.386973</td>\n",
              "      <td>1.823242</td>\n",
              "      <td>0.467554</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.417556</td>\n",
              "      <td>1.654498</td>\n",
              "      <td>0.494414</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.376440</td>\n",
              "      <td>1.650849</td>\n",
              "      <td>0.494414</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8UY3bfOb-jA",
        "outputId": "a4725425-e091-4290-eadd-b132bd36dc21"
      },
      "source": [
        "#@ MODEL EVALUATION: \n",
        "n, counts = 0, torch.zeros(len(vocab))                          # Initialization. \n",
        "for x, y in dls.valid: \n",
        "    n += y.shape[0]\n",
        "    for i in range_of(vocab):\n",
        "        counts[i] += (y==i).long().sum()\n",
        "idx = torch.argmax(counts)                                      # Common Tensor Index. \n",
        "idx, vocab[idx.item()], counts[idx].item()/n                    # Inspection. "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(29), 'thousand', 0.15165200855716662)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSCCpplwGHFH"
      },
      "source": [
        "**RECURRENT NEURAL NETWORK:**\n",
        "- I will simplify the neural networks model or module defined above by replacing the duplicated code that calls the layers with a for loop. The module can work equally to token sequences of different lengths. **Recurrent Neural Network** is a network which is a refactoring of a multilayer neural network using for loop.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezGE9zpcfoQp"
      },
      "source": [
        "#@ LANGUAGE MODEL IN PYTORCH: RECURRENT NEURAL NETWORK:\n",
        "class LMModel2(Module):                                         # Defining Language Model Class.\n",
        "    def __init__(self, vocab_sz, num_hidden):                   # Initializing Constructor Function. \n",
        "        self.i_h = nn.Embedding(vocab_sz, num_hidden)           # Initializing Embedding Layer. \n",
        "        self.h_h = nn.Linear(num_hidden, num_hidden)            # Initializing Linear Layer. \n",
        "        self.h_o = nn.Linear(num_hidden, vocab_sz)              # Initializing Linear Layer. \n",
        "    \n",
        "    def forward(self, x):                                       # Forward Propagation Function.\n",
        "        h = 0                                                   # Initializing Activations. \n",
        "        for i in range(3):\n",
        "            h = h + self.i_h(x[:, i])                           # Initializing Word Embeddings. \n",
        "            h = F.relu(self.h_h(h))                             # Implementation of RELU. \n",
        "        return self.h_o(h)                                      # Implementation of Linear Layer. "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-Rr8bVDJ-7z"
      },
      "source": [
        "**Hidden State:**\n",
        "- **Hidden State** is defined as the activations that are updated at each step of a recurrent neural network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "Q_kYWM5ZJYu_",
        "outputId": "b15bc364-0491-4997-b529-a1ac112f5133"
      },
      "source": [
        "#@ TRAINING THE LANGUAGE MODEL: \n",
        "learn = Learner(dls, LMModel2(len(vocab), 64),                  # Initializing Learner with Language Model. \n",
        "                loss_func=F.cross_entropy, metrics=accuracy)    # Initializing Cross Entropy Loss Function. \n",
        "learn.fit_one_cycle(4, 1e-3)                                    # Training the Model. "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.816274</td>\n",
              "      <td>1.964143</td>\n",
              "      <td>0.460185</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.423805</td>\n",
              "      <td>1.739964</td>\n",
              "      <td>0.473259</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.430327</td>\n",
              "      <td>1.685172</td>\n",
              "      <td>0.485382</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.388390</td>\n",
              "      <td>1.657033</td>\n",
              "      <td>0.470406</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjtXg9_YQPLa"
      },
      "source": [
        "**IMPROVING RECURRENT NEURAL NETWORK:**\n",
        "- I will define a stateful **Recurrent Neural Network** as it remembers its activations between different calls to forward which represents its use for different samples in the batch. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFT3ZigrJhsv"
      },
      "source": [
        "#@ LANGUAGE MODEL IN PYTORCH: RECURRENT NEURAL NETWORK:\n",
        "class LMModel3(Module):                                         # Defining Language Model Class.\n",
        "    def __init__(self, vocab_sz, num_hidden):                   # Initializing Constructor Function. \n",
        "        self.i_h = nn.Embedding(vocab_sz, num_hidden)           # Initializing Embedding Layer. \n",
        "        self.h_h = nn.Linear(num_hidden, num_hidden)            # Initializing Linear Layer. \n",
        "        self.h_o = nn.Linear(num_hidden, vocab_sz)              # Initializing Linear Layer. \n",
        "        self.h = 0                                              # Initializing Hidden State. \n",
        "    \n",
        "    def forward(self, x):                                       # Forward Propagation Function. \n",
        "        for i in range(3):\n",
        "            self.h = self.h + self.i_h(x[:, i])                 # Initializing Word Embeddings. \n",
        "            self.h = F.relu(self.h_h(self.h))                   # Implementation of RELU. \n",
        "        out = self.h_o(self.h)                                  # Implementation of Linear Layer. \n",
        "        self.h = self.h.detach()                                # Removing all the Gradients History. \n",
        "        return out \n",
        "    \n",
        "    def reset(self): self.h = 0                                 # Defining Reset Function. "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNOTRjO5UnP3"
      },
      "source": [
        "**BACKPROPAGATION THROUGH TIME:**\n",
        "- **Backpropagation through Time** is a process of treating a neural network with effectively one layer per time step as one big model and calculating gradients on it in the usual way. The **BPTT** technique is used to avoid running out of memory and time which detaches the history of computation steps in the hidden state every few time steps. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVjxZvQoUfEq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}