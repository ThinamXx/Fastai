{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LanguageModel.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "GAJldAyzepOb"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAJldAyzepOb"
      },
      "source": [
        "### **INITIALIZATION:**\n",
        "- I use these three lines of code on top of my each notebooks because it will help to prevent any problems while reloading the same project. And the third line of code helps to make visualization within the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VO53Px9xdrmE"
      },
      "source": [
        "#@ INITIALIZATION: \n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk8Co18Leyiy"
      },
      "source": [
        "**LIBRARIES AND DEPENDENCIES:**\n",
        "- I have downloaded all the libraries and dependencies required for the project in one particular cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0I-gBJ1ewPX"
      },
      "source": [
        "#@ INSTALLING DEPENDENCIES: UNCOMMENT BELOW: \n",
        "# !pip install -Uqq fastbook\n",
        "# import fastbook\n",
        "# fastbook.setup_book()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l9ySLgOe5QN"
      },
      "source": [
        "#@ DOWNLOADING LIBRARIES AND DEPENDENCIES: \n",
        "from fastbook import *                              # Getting all the Libraries. \n",
        "from fastai.callback.fp16 import *\n",
        "from fastai.text.all import *                       # Getting all the Libraries."
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCAIhIx-fhYh"
      },
      "source": [
        "### **GETTING THE DATA:**\n",
        "- I will use **Human Numbers** dataset here. It contains the first 10000 numbers written out in English. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcPI-9djfVt2",
        "outputId": "eaec4ba5-9554-4f57-a346-c50d5b5b9dce"
      },
      "source": [
        "#@ GETTING THE DATASET: \n",
        "path = untar_data(URLs.HUMAN_NUMBERS)               # Path to the Dataset. \n",
        "path.ls()                                           # Inspecting the Dataset. "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#2) [Path('/root/.fastai/data/human_numbers/train.txt'),Path('/root/.fastai/data/human_numbers/valid.txt')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxZ4-F9EgOpz",
        "outputId": "bf070f21-7e83-47dd-f6eb-0fa18968ba8d"
      },
      "source": [
        "#@ JOINING AND INSPECTING THE DATASET: \n",
        "lines = L()                                         # Initializing a List. \n",
        "with open(path/\"train.txt\") as f:                   # Opening the File. \n",
        "    lines += L(*f.readlines())                      # Reading the Lines. \n",
        "with open(path/\"valid.txt\") as f:                   # Opening the File. \n",
        "    lines += L(*f.readlines())                      # Reading the Lines. \n",
        "lines                                               # Inspection. "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#9998) ['one \\n','two \\n','three \\n','four \\n','five \\n','six \\n','seven \\n','eight \\n','nine \\n','ten \\n'...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xUisxqHRk16e",
        "outputId": "a9a09d0d-629b-45fa-f44f-58c57735a65b"
      },
      "source": [
        "#@ PREPARING THE DATASET: \n",
        "text = \" . \".join([l.strip() for l in lines])       # Preparing the Dataset. \n",
        "text[:100]                                          # Inspection. "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'one . two . three . four . five . six . seven . eight . nine . ten . eleven . twelve . thirteen . fo'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glIUHjTDl69L"
      },
      "source": [
        "**Note:**\n",
        "- I will tokenize the dataset by splitting on spaces. Then I will create a list of unique tokens called vocab for **Numericalization**. Then I will convert the tokens into numbers by looking up in the index of each in the vocab. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNqUHDHEli9w",
        "outputId": "13afcef6-c8b8-4830-88be-0e647d58bcc6"
      },
      "source": [
        "#@ TOKENIZING THE DATASET: \n",
        "tokens = text.split(\" \")                            # Splitting into Tokens. \n",
        "tokens[:10]                                         # Inspecting Tokens. "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['one', '.', 'two', '.', 'three', '.', 'four', '.', 'five', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRnH-KHImgIA",
        "outputId": "49472b36-7b93-458d-ce8e-6dcd75b89f37"
      },
      "source": [
        "#@ GETTING UNIQUE TOKENS: \n",
        "vocab = L(*tokens).unique()                         # Getting Unique Tokens. \n",
        "vocab                                               # Inspection. "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#30) ['one','.','two','three','four','five','six','seven','eight','nine'...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZotFzTfhnCTI",
        "outputId": "6f3e4e6d-b461-4b10-f3d7-a090b379d6ed"
      },
      "source": [
        "#@ CONVERTING TOKENS INTO NUMBERS: \n",
        "word2idx = {w:i for i,w in enumerate(vocab)}       # Getting Index of Tokens. \n",
        "nums = L(word2idx[i] for i in tokens)              # Converting into Numbers. \n",
        "nums                                               # Inspection.  "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#63095) [0,1,2,1,3,1,4,1,5,1...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_a1MXbJNo0St"
      },
      "source": [
        "### **LANGUAGE MODEL FROM SCRATCH:**\n",
        "- Here I will create a list of every sequence of three words as independent variables and the next word after each sequence as the dependent variable. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7gbp6KZoHpv",
        "outputId": "f7cb77bf-972c-4466-a430-ac6e9e662763"
      },
      "source": [
        "#@ CREATING SEQUENCE OF TOKENS: \n",
        "L((tokens[i:i+3], tokens[i+3]) for i in range(0, len(tokens)-4, 3))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#21031) [(['one', '.', 'two'], '.'),(['.', 'three', '.'], 'four'),(['four', '.', 'five'], '.'),(['.', 'six', '.'], 'seven'),(['seven', '.', 'eight'], '.'),(['.', 'nine', '.'], 'ten'),(['ten', '.', 'eleven'], '.'),(['.', 'twelve', '.'], 'thirteen'),(['thirteen', '.', 'fourteen'], '.'),(['.', 'fifteen', '.'], 'sixteen')...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxKUTFqoqalx",
        "outputId": "532d60c3-6c2f-4b77-f4d6-9c85065e43c6"
      },
      "source": [
        "#@ CREATING SEQUENCE OF TENSORS FOR NUMERICALIZED VALUES: \n",
        "seqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0, len(nums)-4, 3))   # Creating Sequence.\n",
        "seqs                                                                           # Inspection.  "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#21031) [(tensor([0, 1, 2]), 1),(tensor([1, 3, 1]), 4),(tensor([4, 1, 5]), 1),(tensor([1, 6, 1]), 7),(tensor([7, 1, 8]), 1),(tensor([1, 9, 1]), 10),(tensor([10,  1, 11]), 1),(tensor([ 1, 12,  1]), 13),(tensor([13,  1, 14]), 1),(tensor([ 1, 15,  1]), 16)...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q--On8YkUWSD"
      },
      "source": [
        "#@ CREATING DATALOADERS: \n",
        "bs = 64                                                 # Initializing Batchsize. \n",
        "cut = int(len(seqs) * 0.8)                              # Initialization. \n",
        "dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], \n",
        "                             bs=bs, shuffle=False)      # Initializing Data Loaders. "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcUWBI0PVpOP"
      },
      "source": [
        "**LANGUAGE MODEL:**\n",
        "- I will create neural network architecture that takes three words as input and returns the predictions of the probability of each possible next word in the vocab. I will use three standard linear layers. The first linear layer will use only the first words embedding as activations. The second layer will use the second words embedding plus the first layers output activations and the third layer will use the third words embedding plus the second layers output activations. The key effect is that every word is interpreted in the information context of any words preceding it. Each of these three layers will use the same weight matrix. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-Djq4aoVoj7"
      },
      "source": [
        "#@ LANGUAGE MODEL IN PYTORCH: SIMPLE LINEAR MODEL: \n",
        "class LMModel1(Module):                                         # Defining Language Model Class. \n",
        "    def __init__(self, vocab_sz, n_hidden):                     # Initializing Constructor Function. \n",
        "        self.i_h = nn.Embedding(vocab_sz, n_hidden)             # Initializing Embedding Layer. \n",
        "        self.h_h = nn.Linear(n_hidden, n_hidden)                # Initializing Linear Layer. \n",
        "        self.h_o = nn.Linear(n_hidden, vocab_sz)                # Initializing Linear Layer. \n",
        "    \n",
        "    def forward(self, x):                                       # Forward Propagation Function. \n",
        "        h = F.relu(self.h_h(self.i_h(x[:, 0])))                 # Implementation of RELU. \n",
        "        h = h + self.i_h(x[:, 1])                               # Second Word Embeddings and Activations. \n",
        "        h = F.relu(self.h_h(h))                                 # Implementation of RELU. \n",
        "        h = h + self.i_h(x[:, 2])                               # Third Word Embeddings and Activations. \n",
        "        h = F.relu(self.h_h(h))                                 # Implementation of RELU. \n",
        "        return self.h_o(h)                                      # Implementation of Linear Layer. "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "cUzzGOm7bAzV",
        "outputId": "ec116410-25c4-4783-cea9-f8a8821f2133"
      },
      "source": [
        "#@ TRAINING THE LANGUAGE MODEL: \n",
        "learn = Learner(dls, LMModel1(len(vocab), 64),                  # Initializing Learner with Language Model. \n",
        "                loss_func=F.cross_entropy, metrics=accuracy)    # Initializing Cross Entropy Loss Function. \n",
        "learn.fit_one_cycle(4, 1e-3)                                    # Training the Model. "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.824297</td>\n",
              "      <td>1.970941</td>\n",
              "      <td>0.467554</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.386973</td>\n",
              "      <td>1.823242</td>\n",
              "      <td>0.467554</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.417556</td>\n",
              "      <td>1.654498</td>\n",
              "      <td>0.494414</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.376440</td>\n",
              "      <td>1.650849</td>\n",
              "      <td>0.494414</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8UY3bfOb-jA",
        "outputId": "4c2811b5-0546-4345-a326-bb45550bd12d"
      },
      "source": [
        "#@ MODEL EVALUATION: \n",
        "n, counts = 0, torch.zeros(len(vocab))                          # Initialization. \n",
        "for x, y in dls.valid: \n",
        "    n += y.shape[0]\n",
        "    for i in range_of(vocab):\n",
        "        counts[i] += (y==i).long().sum()\n",
        "idx = torch.argmax(counts)                                      # Common Tensor Index. \n",
        "idx, vocab[idx.item()], counts[idx].item()/n                    # Inspection. "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(29), 'thousand', 0.15165200855716662)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pKGllhGeUya"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}