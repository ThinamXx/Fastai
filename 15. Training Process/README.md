# **Fastai : Training Process**

The [**Training**](https://github.com/ThinamXx/Fastai/blob/main/15.%20Training%20Process/Training.ipynb) notebook contains all the dependencies required to create a training loop and explored variants of Stochastic Gradient Descent. 

**Note:**
- ðŸ“‘[**Training Process**](https://nbviewer.jupyter.org/github/ThinamXx/Fastai/blob/main/15.%20Training%20Process/Training.ipynb)

**Preparing the Dataset**
- I have presented the implementation of Preparing Dataset and Baseline Model using Fastai and PyTorch here in the snapshot. 

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%20259.PNG)
![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%20260.PNG)

**Adam Optimizer**
- Adam mixes the idea of SGD with momentum and RMSProp together where it uses the moving average of the gradients as a direction and divides by the square root of the moving average of the gradients squared to give an adaptive learning rate to each parameter. It takes the unbiased moving average. I have presented the implementation of RMS Prop and Adam Optimizer using Fastai and PyTorch here in the snapshot.

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%20262.PNG)
![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%20263.PNG)
