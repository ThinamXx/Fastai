{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralFoundations.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlHj7lUPa187"
      },
      "source": [
        "**INITIALIZATION**\n",
        "- I use these three lines of code on top of my each notebooks because it will help to prevent any problems while reloading the same project. And the third line of code helps to make visualization within the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taFeJgiOafpU"
      },
      "source": [
        "#@ INITIALIZATION: \n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxH0FCEga_zV"
      },
      "source": [
        "**LIBRARIES AND DEPENDENCIES**\n",
        "- I have downloaded all the libraries and dependencies required for the project in one particular cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4v7GAR2a-IJ"
      },
      "source": [
        "#@ INSTALLING DEPENDENCIES: UNCOMMENT BELOW: \n",
        "# !pip install -Uqq fastbook\n",
        "# import fastbook\n",
        "# fastbook.setup_book()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk9RqKhcbJor"
      },
      "source": [
        "#@ DOWNLOADING LIBRARIES AND DEPENDENCIES: \n",
        "from fastbook import *                                  # Getting all the Libraries. \n",
        "from fastai.callback.fp16 import *\n",
        "from torch.autograd import Function"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi-60nNpbv1b"
      },
      "source": [
        "**MODELING NEURON**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4urwjTeBbeHB"
      },
      "source": [
        "#@ MODELING NEURON FROM SCRATCH: UNCOMMENT BELOW: \n",
        "# output = sum([x*w for x,w in zip(inputs, weights)]) + bias          # Adding Weighted Inputs and Bias. \n",
        "# def relu(x): return x if x >= 0 else 0                              # Defining RELU Activation Function. \n",
        "# y[i, j] = sum([a*b for a,b in zip(x[i,:], w[j,:])]) + b[j]          # Initializing Matrix Multiplication. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYCUMtyJgXza",
        "outputId": "c27f0024-89af-4113-9f65-66b06f250a79"
      },
      "source": [
        "#@ MATRIX MULTIPLICATION FROM SCRATCH: \n",
        "def matmul(a, b):                            # Defining Matrix Multiplication Function. \n",
        "    ar, ac = a.shape                         # Inspecting Shape. \n",
        "    br, bc = b.shape                         # Inspecting Shape. \n",
        "    assert ac == br                          # Asserting Rows and Columns. \n",
        "    c = torch.zeros(ar, bc)                  # Initializing Tensor. \n",
        "    for i in range(ar):                      # Getting Row Indices. \n",
        "        for j in range(bc):                  # Getting Column Indices. \n",
        "            for k in range(ac):              # Getting Inner Sum. \n",
        "                c[i,j] += a[i,k] * b[k,j]    # Getting Matrix Multiplication. \n",
        "    return c\n",
        "\n",
        "#@ IMPLEMENTATION OF MATRIX MULTIPLICATION: \n",
        "m1 = torch.randn(5, 28*28)                   # Initializing Matrix. \n",
        "m2 = torch.randn(784, 10)                    # Initializing Matrix. \n",
        "%time t1 = matmul(m1, m2)                    # Implementation of Matrix Multiplication. \n",
        "%timeit -n 20 t2 = m1@m2                     # Implementation of Matrix Multiplication. "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 659 ms, sys: 0 ns, total: 659 ms\n",
            "Wall time: 731 ms\n",
            "The slowest run took 142.02 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "20 loops, best of 5: 7.82 µs per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CY-ny3EzjFXX",
        "outputId": "4e390293-78b6-4077-c2eb-b3d307060f19"
      },
      "source": [
        "#@ ELEMENTWISE ARITHMETIC: \n",
        "a = tensor([10., 6, -4])                     # Initializing a Tensor. \n",
        "b = tensor([2., 8, 7])                       # Initializing a Tensor. \n",
        "a + b                                        # Initializing Elementwise Addition. \n",
        "(a<b).all(), (a==b).all()                    # Combining Elementwise Operations. \n",
        "(a + b).sum().item()                         # Converting Tensors. "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29.0"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXImxI3msHT0",
        "outputId": "84a7ee3b-12a6-4df6-a680-b84888824ed8"
      },
      "source": [
        "#@ MATRIX MULTIPLICATION: SIMPLIFIED: \n",
        "def matmul(a, b):                            # Defining Matrix Multiplication Function. \n",
        "    ar, ac = a.shape                         # Inspecting Shape. \n",
        "    br, bc = b.shape                         # Inspecting Shape. \n",
        "    assert ac == br                          # Asserting Rows and Columns. \n",
        "    c = torch.zeros(ar, bc)                  # Initializing Tensor. \n",
        "    for i in range(ar):                      # Getting Row Indices. \n",
        "        for j in range(bc):                  # Getting Column Indices. \n",
        "            c[i,j] = (a[i] * b[:,j]).sum()\n",
        "    return c\n",
        "\n",
        "#@ IMPLEMENTATION OF MATRIX MULTIPLICATION: \n",
        "m1 = torch.randn(5, 28*28)                   # Initializing Matrix. \n",
        "m2 = torch.randn(784, 10)                    # Initializing Matrix. \n",
        "%time t1 = matmul(m1, m2)                    # Implementation of Matrix Multiplication. "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.76 ms, sys: 0 ns, total: 1.76 ms\n",
            "Wall time: 1.87 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkxhOb3TtTdJ",
        "outputId": "035a3dc0-d1b6-4dec-a0b8-64914c8abffe"
      },
      "source": [
        "#@ BROADCASTING VECTOR TO A MATRIX: \n",
        "c = tensor([10., 20, 30])                    # Initializing a Tensor. \n",
        "m = tensor([[1.,2,3], [4,5,6], [7,8,9]])     # Initializing a Tensor. \n",
        "m.shape, c.shape                             # Inspecting Tensors. "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 3]), torch.Size([3]))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acHR6ETDu4Uc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f29d6a53-c737-48b2-9936-409939906b8a"
      },
      "source": [
        "#@ IMPLEMENTATION OF BROADCASTING: \n",
        "c = tensor([10., 20, 30])                    # Initializing a Tensor. \n",
        "m = tensor([[1.,2,3], [4,5,6], [7,8,9]])     # Initializing a Tensor. \n",
        "c = c.unsqueeze(1)                           # Adding a Unit Dimension. \n",
        "m.shape, c.shape                             # Inspecting Tensors. "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 3]), torch.Size([3, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Elk0-z5X-XnQ",
        "outputId": "44e8a66c-3be0-4577-d186-ee9a873cd427"
      },
      "source": [
        "#@ MATRIX MULTIPLICATION: SIMPLIFIED: \n",
        "def matmul(a, b):                            # Defining Matrix Multiplication Function. \n",
        "    ar, ac = a.shape                         # Inspecting Shape. \n",
        "    br, bc = b.shape                         # Inspecting Shape. \n",
        "    assert ac == br                          # Asserting Rows and Columns. \n",
        "    c = torch.zeros(ar, bc)                  # Initializing Tensor. \n",
        "    for i in range(ar):                      # Getting Row Indices. \n",
        "        c[i]=(a[i].unsqueeze(-1) * b\n",
        "              ).sum(dim=0)\n",
        "    return c\n",
        "\n",
        "#@ IMPLEMENTATION OF MATRIX MULTIPLICATION: \n",
        "m1 = torch.randn(5, 28*28)                   # Initializing Matrix. \n",
        "m2 = torch.randn(784, 10)                    # Initializing Matrix. \n",
        "%time t1 = matmul(m1, m2)                    # Implementation of Matrix Multiplication. \n",
        "%timeit -n 20 t4 = m1@m2                     # Implementation of Matrix Multiplication. "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 671 µs, sys: 0 ns, total: 671 µs\n",
            "Wall time: 9.57 ms\n",
            "The slowest run took 9.73 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "20 loops, best of 5: 5.76 µs per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJwFTSqtC_iY"
      },
      "source": [
        "**EINSTEIN SUMMATION**\n",
        "- Einstein Summation is a compact representation for combining products and sums. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dUPhMn5_laQ",
        "outputId": "fbd9ab4d-89dc-43bd-ca05-99e349b914e9"
      },
      "source": [
        "#@ EINSTEIN SUMMATION IMPLEMENTATION: \n",
        "def matmul(a, b):                            # Defining Matrix Multiplication Function. \n",
        "    return torch.einsum(\"ik,kj->ij\",a,b)     # Implementation of Einstein Summation. \n",
        "%timeit -n 20 t5 = m1@m2                     # Implementation of Matrix Multiplication. "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20 loops, best of 5: 7.12 µs per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKRcwfi0D_72"
      },
      "source": [
        "**FORWARD AND BACKWARD PASSES**\n",
        "- Computing all the gradients of a given loss with respect to its parameters is known as **Backward Pass**. Similarly computing the output of the model on a given input based on the matrix products is known as **Forward Pass**. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdwHrYUQDqyz",
        "outputId": "724f09dc-8096-446b-b61e-6eb850226b9f"
      },
      "source": [
        "#@ DEFINING AND INITIALIZING LAYER: \n",
        "def lin(x, w, b): return x @ w + b                  # Defining Linear Layer. \n",
        "x = torch.randn(200, 100)                           # Initializing Random Input Tensors. \n",
        "y = torch.randn(200)                                # Initializing Random Output Tensors. \n",
        "w1 = torch.randn(100, 50)                           # Initializing Random Weights. \n",
        "b1 = torch.zeros(50)                                # Initializing Random Bias. \n",
        "w2 = torch.randn(50, 1)                             # Initializing Random Weights. \n",
        "b2 = torch.zeros(1)                                 # Initializing Random Bias. \n",
        "\n",
        "#@ IMPLEMENTATION OF LINEAR FUNCTION: \n",
        "l1 = lin(x, w1, b1)                                 # Implementation of Function. \n",
        "l1.shape                                            # Inspecting the Shape. "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([200, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFJC1NyH_IFe",
        "outputId": "8e5b52a2-c60a-4fdc-8367-35d3181c2d5a"
      },
      "source": [
        "#@ INSPECTING MEAN AND STANDARD DEVIATION: \n",
        "l1.mean(), l1.std()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-0.0569), tensor(10.1162))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myV0eK0r_Vma",
        "outputId": "01845b7e-a237-490a-e6ed-fca8615a8aeb"
      },
      "source": [
        "#@ UNDERSTANDING MATRIX MULTIPLICATIONS: \n",
        "x = torch.randn(200, 100)                            # Initializing Random Numbers. \n",
        "for i in range(50):\n",
        "    x = x @ torch.randn(100, 100)                    # Initializing Matrix Multiplications. \n",
        "x[0:5, 0:5]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[nan, nan, nan, nan, nan],\n",
              "        [nan, nan, nan, nan, nan],\n",
              "        [nan, nan, nan, nan, nan],\n",
              "        [nan, nan, nan, nan, nan],\n",
              "        [nan, nan, nan, nan, nan]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNrC5a1rAU5n",
        "outputId": "dfcd3b4d-a4a2-48b0-9ea3-b06a756be00d"
      },
      "source": [
        "#@ UNDERSTANDING MATRIX MULTIPLICATIONS: \n",
        "x = torch.randn(200, 100)                            # Initializing Random Numbers. \n",
        "for i in range(50):\n",
        "    x = x @ (torch.randn(100, 100) * 0.01)           # Initializing Matrix Multiplications. \n",
        "x[0:5, 0:5]                                          # Inspection. "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPsXfYvLAw7f",
        "outputId": "481f3091-cef8-4dd7-a18e-ed722be26ffd"
      },
      "source": [
        "#@ UNDERSTANDING MATRIX MULTIPLICATIONS: \n",
        "x = torch.randn(200, 100)                            # Initializing Random Numbers. \n",
        "for i in range(50):\n",
        "    x = x @ (torch.randn(100, 100) * 0.1)            # Initializing Matrix Multiplications. \n",
        "x[0:5, 0:5]                                          # Inspection. "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.5783e-02,  5.1167e-01,  1.0287e-03,  1.4076e-01, -1.0012e+00],\n",
              "        [ 5.4434e-01,  1.2963e+00,  1.0347e+00,  1.5280e+00, -9.2810e-01],\n",
              "        [-6.0311e-01, -1.0227e+00, -3.7155e-01, -6.7006e-01,  5.1546e-01],\n",
              "        [-1.4638e-01, -7.7870e-01, -4.5685e-01, -9.4609e-01,  4.8461e-01],\n",
              "        [-3.0557e-01, -5.9048e-02, -8.6489e-02, -4.2007e-01, -3.3071e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YsTAz_MBSpX",
        "outputId": "dea6b7a2-10bb-4e36-b352-6164b844808a"
      },
      "source": [
        "#@ IMPLEMENTATION OF XAVIER INITIALIZATION: \n",
        "x = torch.randn(200, 100)                           # Initializing Random Input Tensors. \n",
        "y = torch.randn(200)                                # Initializing Random Output Tensors. \n",
        "w1 = torch.randn(100, 50) / math.sqrt(100)          # Initializing Random Weights. \n",
        "b1 = torch.zeros(50)                                # Initializing Random Bias. \n",
        "w2 = torch.randn(50, 1) / math.sqrt(50)             # Initializing Random Weights. \n",
        "b2 = torch.zeros(1)                                 # Initializing Random Bias. \n",
        "\n",
        "#@ IMPLEMENTATION OF LINEAR FUNCTION: \n",
        "l1 = lin(x, w1, b1)                                 # Implementation of Function. \n",
        "l1.mean(), l1.std()                                 # Inspection. "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.0013), tensor(1.0043))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3kcpDdgDVAh",
        "outputId": "3ff290ce-944f-40bc-b4de-62e1d828c975"
      },
      "source": [
        "#@ IMPLEMENTATION OF RELU ACTIVATION FUNCTION: \n",
        "def relu(x):                                        # Defining Relu Function. \n",
        "    return x.clamp_min(0.)                          # Replacing Negatives with Zeros. \n",
        "l2 = relu(l1)                                       # Implementation of RELU. \n",
        "l2.mean(), l2.std()                                 # Inspection. "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.3996), tensor(0.5879))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dS_SrN-Ew1B",
        "outputId": "0abfb9da-63e5-4e0d-bc5e-2b6e847a3f9a"
      },
      "source": [
        "#@ MATRIX MULTIPLICATIONS AND RELU: \n",
        "x = torch.randn(200, 100)                            # Initializing Random Numbers. \n",
        "for i in range(50):\n",
        "    x = relu(x @ (torch.randn(100, 100) * 0.1))      # Initializing Matrix Multiplications. \n",
        "x[0:5, 0:5]                                          # Inspection. "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.3917e-08, 1.9313e-09, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
              "        [1.1168e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
              "        [2.0183e-08, 1.0267e-09, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
              "        [2.0421e-08, 9.1188e-10, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
              "        [1.1986e-08, 1.4588e-09, 0.0000e+00, 0.0000e+00, 0.0000e+00]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5jC9JL9FHEM",
        "outputId": "8dc80f39-e990-4c86-9957-5dc8168c0708"
      },
      "source": [
        "#@ MATRIX MULTIPLICATIONS AND RELU: \n",
        "x = torch.randn(200, 100)                            # Initializing Random Numbers. \n",
        "for i in range(50):\n",
        "    x = relu(x @ (torch.randn(100, 100) * \n",
        "                  math.sqrt(2/100)))                 # Initializing Matrix Multiplications. \n",
        "x[0:5, 0:5]                                          # Inspection. "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4409, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [1.7408, 0.0000, 0.0000, 0.4863, 0.1915],\n",
              "        [1.0620, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.7964, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3552, 0.0000, 0.0000, 0.0000, 0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bdP5IjeDF_y",
        "outputId": "fd1ea971-5bf9-4b4a-d222-6a9a8183848c"
      },
      "source": [
        "#@ IMPLEMENTATION OF KAIMING INITILIZATION: \n",
        "x = torch.randn(200, 100)                             # Initializing Random Input Tensors.\n",
        "y = torch.randn(200)                                  # Initializing Random Output Tensors.\n",
        "w1 = torch.randn(100, 50) * math.sqrt(2/100)          # Initializing Weight Tensors. \n",
        "b1 = torch.zeros(50)                                  # Initializing Bias. \n",
        "w2 = torch.rand(50, 1) * math.sqrt(2/50)              # Initializing Weight Tensors. \n",
        "b2 = torch.zeros(1)                                   # Initializing Bias. \n",
        "\n",
        "#@ IMPLEMENTATION OF LINEAR LAYER AND RELU: \n",
        "l1 = lin(x, w1, b1)                                   # Implementation of Linear Function. \n",
        "l2 = relu(l1)                                         # Implementation of RELU. \n",
        "l2.mean(), l2.std()                                   # Inspection. "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.5623), tensor(0.8257))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqsAoiwfFBbx",
        "outputId": "9b374677-767d-4025-aa22-7bf76cbd7970"
      },
      "source": [
        "#@ DEFINING MODEL: \n",
        "def model(x):                                         # Defining Model Function. \n",
        "    l1 = lin(x, w1, b1)                               # Implementation of Linear Function. \n",
        "    l2 = relu(l1)                                     # Implementation of RELU Function. \n",
        "    l3 = lin(l2, w2, b2)                              # Implementation of Linear Function. \n",
        "    return l3\n",
        "\n",
        "#@ IMPLEMENTATION OF MODEL: \n",
        "out = model(x)                                        # Initializing the Model. \n",
        "out.shape                                             # Inspection. "
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([200, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkbtFm71GJ9C"
      },
      "source": [
        "#@ DEFINING LOSS FUNCTION: \n",
        "def mse(output, targ):                                # Defining Loss Function. \n",
        "    return (output.squeeze(-1)-targ).pow(2).mean()    # Initializing Mean Squared Error. \n",
        "loss = mse(out, y)                                    # Inspecting Loss. "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRNluxaAG31R"
      },
      "source": [
        "#@ GRADIENT OF LOSS FUNCTION: \n",
        "def mse_grad(inp, targ):                              # Defining Gradient Function. \n",
        "    inp.g = 2. * (inp.squeeze() - targ\n",
        "                  ).unsqueeze(-1) / inp.shape[0]      # Calculating Gradients of Loss. \n",
        "\n",
        "#@ GRADIENT OF RELU ACTIVATION FUNCTION: \n",
        "def relu_grad(inp, out):                              # Defining Gradient Function. \n",
        "    inp.g = (inp>0).float() * out.g                   # Calculating Gradients. "
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPBckm1ZUJFS"
      },
      "source": [
        "#@ GRADIENT OF MATRIX MULTIPLICATION: \n",
        "def lin_grad(inp, out, w, b):                         # Defining the Function. \n",
        "    inp.g = out.g @ w.t()                             # Getting the Gradients of Input. \n",
        "    w.g = inp.t() @ out.g                             # Getting the Gradients of Weight. \n",
        "    b.g = out.g.sum(0)                                # Getting the Gradients of Bias. \n",
        "\n",
        "#@ FORWARD AND BACKWARD PROPAGATION FUNCTION: \n",
        "def forward_and_backward(inp, targ):                  # Defining the Function. \n",
        "    l1 = inp @ w1 + b1                                # Initializing Linear Layer. \n",
        "    l2 = relu(l1)                                     # Implementation of RELU. \n",
        "    out = l2 @ w2 + b2                                # Initializing Linear Layer. \n",
        "    loss = mse(out, targ)                             # Initializing Loss Function. \n",
        "\n",
        "    mse_grad(out, targ)                               # Calculating Gradients of Loss. \n",
        "    lin_grad(l2, out, w2, b2)                         # Getting Gradients. \n",
        "    relu_grad(l1, l2)                                 # Getting Gradients. \n",
        "    lin_grad(inp, l1, w1, b1)                         # Getting Gradients. "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6Yw7y7sfJLJ"
      },
      "source": [
        "**REFACTORING THE MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1rGohYYfEbd"
      },
      "source": [
        "#@ INITIALIZING RELU CLASS: \n",
        "class Relu():                                         # Defining RELU Class. \n",
        "    def __call__(self, inp):                          # Initializing Callable Function. \n",
        "        self.inp = inp                                # Initialization. \n",
        "        self.out = inp.clamp_min(0.)                  # Initialization. \n",
        "        return self.out\n",
        "    \n",
        "    def backward(self):                               # Backward Propagation Function. \n",
        "        self.inp.g = (self.inp>0).float()*self.out.g  # Getting Gradients of Inputs. "
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoDv4LE6ibk9"
      },
      "source": [
        "#@ INITIALIZING LINEAR CLASS: \n",
        "class Lin():                                          # Defining Linear Class. \n",
        "    def __init__(self, w, b):                         # Initializing Constructor Function. \n",
        "        self.w, self.b = w, b                         # Initialization. \n",
        "    \n",
        "    def __call__(self, inp):                          # Initializing Callable Function. \n",
        "        self.inp = inp                                # Initializing Inputs. \n",
        "        self.out = inp @ self.w + self.b              # Initializing Outputs. \n",
        "        return self.out\n",
        "    \n",
        "    def backward(self):                               # Backward Propagation Function. \n",
        "        self.inp.g = self.out.g @ self.w.t()          # Calculating Gradients of Inputs. \n",
        "        self.w.g = self.inp.t() @ self.out.g          # Calculating Gradients of Weights.\n",
        "        self.b.g = self.out.g.sum(0)                  # Calculating Gradients of Bias.  "
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vM7lqXB9koyc"
      },
      "source": [
        "#@ INITIALIZING MEAN SQUARED ERROR: \n",
        "class Mse():                                          # Defining MSE Class. \n",
        "    def __call__(self, inp, targ):                    # Initializing Callable Function. \n",
        "        self.inp = inp                                # Initializing Inputs. \n",
        "        self.targ = targ                              # Initializing Targets. \n",
        "        self.out = (inp.squeeze()-targ).pow(2).mean() # Initializing Outputs. \n",
        "        return self.out \n",
        "    \n",
        "    def backward(self):                               # Backward Propagation Function. \n",
        "        x = (self.inp.squeeze()-self.targ\n",
        "             ).unsqueeze(-1)                          # Calculating Loss. \n",
        "        self.inp.g = 2.*x / self.targ.shape[0]        # Calculating Gradients of Inputs. "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyqfjivVS4TR"
      },
      "source": [
        "#@ DEFINING THE MODEL ARCHITECTURE: \n",
        "class Model():                                              # Defining Model Class. \n",
        "    def __init__(self, w1, b1, w2, b2):                     # Initializing Constructor Function. \n",
        "        self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]      # Initializing Layers of Network. \n",
        "        self.loss = Mse()                                   # Initializing Mean Squared Error Loss. \n",
        "    \n",
        "    def __call__(self, x, targ):                            # Initializing Callable Function. \n",
        "        for l in self.layers: x = l(x)                      # Implementation of Layers of Network. \n",
        "        return self.loss(x, targ)                           # Implementation of Loss Function. \n",
        "\n",
        "    def backward(self):                                     # Initializing Back Propagation Function. \n",
        "        self.loss.backward()                                # Calculating Gradients of Loss. \n",
        "        for l in reversed(self.layers): l.backward()        # Calculating Gradients. \n",
        "\n",
        "#@ IMPLEMENTATION OF MODEL: \n",
        "model = Model(w1, b1, w2, b2)                               # Initializing the Model. \n",
        "loss = model(x, y)                                          # Forward Propagation Function.\n",
        "model.backward()                                            # Back Propagation Function. "
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpUByh32mqvu"
      },
      "source": [
        "**PYTORCH IMPLEMENTATION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "freCczi7lkB6"
      },
      "source": [
        "#@ DEFINING BASE CLASS FUNCTION: \n",
        "class LayerFunction():                                      # Defining Layer Function Class. \n",
        "    def __call__(self, *args):                              # Initializing Callable Function. \n",
        "        self.args = args                                    # Initialization. \n",
        "        self.out = self.forward(*args)                      # Initialization. \n",
        "        return self.out\n",
        "    \n",
        "    def forward(self): raise Exception(\"not implemented\")   # Forward Propagation Function. \n",
        "    def bwd(self): raise Exception(\"not implemented\")       \n",
        "    def backward(self): self.bwd(self.out, *self.args)      # Backward Propagation Function. \n",
        "\n",
        "#@ DEFINING SUBCLASS: RELU FUNCTION: \n",
        "class Relu(LayerFunction):                                  # Defining RELU. \n",
        "    def forward(self, inp): return inp.clamp_min(0.)        # Forward Propagation Function. \n",
        "    def bwd(self, out, inp): inp.g = (inp>0).float()*out.g  # Back Propagation Function. "
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gS5Ig9ZqFg2"
      },
      "source": [
        "#@ DEFINING SUBCLASS: LINEAR LAYER FUNCTION:\n",
        "class Lin(LayerFunction):                                   # Defining Linear Layer Function. \n",
        "    def __init__(self, w, b):                               # Initializing Constructor Function. \n",
        "        self.w, self.b = w, b                               # Initialization. \n",
        "    \n",
        "    def forward(self, inp):                                 # Forward Propagation Function. \n",
        "        return inp @ self.w + self.b                        # Initializing Linear Function. \n",
        "\n",
        "    def bwd(self, out, inp):                                # Initializing Back Propagation. \n",
        "        inp.g = out.g @ self.w.t()                          # Calculating the Gradients. \n",
        "        self.w.g = self.inp.t() @ self.out.g                # Calculating the Gradients. \n",
        "        self.b.g = out.g.sum(0)                             # Calculating the Gradients. \n",
        "\n",
        "#@ DEFINING SUBCLASS: MEAN SQUARED ERROR FUNCTION: \n",
        "class Mse(LayerFunction):                                   # Defining MSE Function. \n",
        "    def forward(self, inp, targ):                           # Forward Propagation Function. \n",
        "        return (inp.squeeze() - targ).pow(2).mean()         # Calculating Mean Squared Error. \n",
        "    \n",
        "    def bwd(self, out, inp, targ):\n",
        "        inp.g = 2*(inp.squeeze() - targ\n",
        "                   ).unsqueeze(-1) / targ.shape[0]          # Initializing Back Propagation. "
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WlzhcvqpGr9"
      },
      "source": [
        "#@ DEFINING CUSTOM FUNCTION: \n",
        "class MyRelu(Function):                                     # Defining RELU Function. \n",
        "    @staticmethod\n",
        "    def forward(ctx, i):                                    # Forward Propagation Function. \n",
        "        result = i.clamp_min(0.)\n",
        "        ctx.save_for_backward(i)\n",
        "        return result\n",
        "    \n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):                         # Back Propagation Function. \n",
        "        i, = ctx.saved_tensors\n",
        "        return grad_output * (i>0).float()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_QYBUWerKHe"
      },
      "source": [
        "**NEURAL NETWORK MODULE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgTNTm_IqvoF",
        "outputId": "b0b29230-38e1-469a-f30d-eedeef95f6c8"
      },
      "source": [
        "#@ DEFINING LINEAR LAYER FROM SCRATCH: \n",
        "class LinearLayer(nn.Module):                               # Defining Linear Layer. \n",
        "    def __init__(self, n_in, n_out):                        # Initializing Constructor Function. \n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(\n",
        "            torch.randn(n_out, n_in) * math.sqrt(2/n_in))   # Initializing Weight Parameters. \n",
        "        self.bias = nn.Parameter(torch.zeros(n_out))        # Initializing Bias Parameters.\n",
        "\n",
        "    def forward(self, x):                                   # Forward Propagation Function.          \n",
        "        return x @ self.weight.t() + self.bias              # Initializing Linear Function. \n",
        "\n",
        "#@ IMPLEMENTATION OF LINEAR LAYER FUNCTION: \n",
        "lin = LinearLayer(10, 2)                                    # Initializing Linear Layer. \n",
        "p1, p2 = lin.parameters()                                   # Initializing Parameters. \n",
        "p1.shape, p2.shape                                          # Insepcting Parameters. "
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2, 10]), torch.Size([2]))"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZevmnMFt-w9"
      },
      "source": [
        "#@ DEFINING LINEAR MODEL: PYTORCH:\n",
        "class Model(nn.Module):                                     # Defining Linear Model. \n",
        "    def __init__(self, n_in, nh, n_out):                    # Initializing Constructor Function. \n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(nn.Linear(n_in, nh),    # Initializing Linear Layer. \n",
        "                                    nn.ReLU(),              # Initializing RELU Function. \n",
        "                                    nn.Linear(nh, n_out))   # Initializing Linear Layer. \n",
        "        self.loss = mse                                     # Initializing MSE Loss Function. \n",
        "    \n",
        "    def forward(self, x, targ):                             # Forward Propagation Function. \n",
        "        return self.loss(self.layers(x).squeeze(), targ)    # Calculating Loss. \n",
        "\n",
        "#@ DEFINING LINEAR MODEL: FASTAI:\n",
        "class Model(Module):                                        # Defining Linear Model. \n",
        "    def __init__(self, n_in, nh, n_out):                    # Initializing Constructor Function. \n",
        "        self.layers = nn.Sequential(nn.Linear(n_in, nh),    # Initializing Linear Layer. \n",
        "                                    nn.ReLU(),              # Initializing RELU Function. \n",
        "                                    nn.Linear(nh, n_out))   # Initializing Linear Layer. \n",
        "    \n",
        "    def forward(self, x, targ):                             # Forward Propagation Function. \n",
        "        return self.loss(self.layers(x).squeeze(), targ)    # Calculating Loss. "
      ],
      "execution_count": 36,
      "outputs": []
    }
  ]
}