# **Fastai : Neural Network Foundations**

The [**Neural Foundations**](https://github.com/ThinamXx/Fastai/blob/main/16.%20Neural%20Network%20Foundations/NeuralFoundations.ipynb) notebook contains all the dependencies required to understand the foundations of deep learning, begining with matrix multiplication and moving on to implementing the forward and backward passes of a neural net from scratch. 

**Note**
- ðŸ“‘[**Neural Network Foundations**](https://nbviewer.jupyter.org/github/ThinamXx/Fastai/blob/main/16.%20Neural%20Network%20Foundations/NeuralFoundations.ipynb)

**Matrix Multiplications**
- I have presented the implementation of Matrix Multiplication from Scratch, Elementwise Arithmetic, Einstein Summation and Defining and Initializing Linear Layer using Fastai and PyTorch here in the snapshot.

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%20264.PNG)
![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%20265.PNG)

**Forward and Backward Passes**
- Computing all the gradients of a given loss with respect to its parameters is known as Backward Pass. Similarly computing the output of the model on a given input based on the matrix products is known as Forward Pass. I have presented the implementation of Kaiming Initialization, MSE Loss Function and Gradients using Fastai and PyTorch here in the snapshot.

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%20268.PNG)

**Linear Model**
- I have presented the implementation of Defining Model Architecture, Layer Function and RELU, Defining Linear Layer and Linear Model using Fastai and PyTorch here in the snapshot.

![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%20269.PNG)
![Image](https://github.com/ThinamXx/300Days__MachineLearningDeepLearning/blob/main/Images/Day%20270.PNG)
