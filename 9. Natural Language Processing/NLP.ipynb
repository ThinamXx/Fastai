{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk5SHfqIc6Ey"
      },
      "source": [
        "### **INITIALIZATION:**\n",
        "- I use these three lines of code on top of my each notebooks because it will help to prevent any problems while reloading the same project. And the third line of code helps to make visualization within the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9K2einKadKr"
      },
      "source": [
        "#@ INITIALIZATION: \n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTFZaPENdHTs"
      },
      "source": [
        "**LIBRARIES AND DEPENDENCIES:**\n",
        "- I have downloaded all the libraries and dependencies required for the project in one particular cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZDkvIrydDtY"
      },
      "source": [
        "#@ INSTALLING DEPENDENCIES: UNCOMMENT BELOW: \n",
        "# !pip install -Uqq fastbook\n",
        "# import fastbook\n",
        "# fastbook.setup_book()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTIdUUtEdOah"
      },
      "source": [
        "#@ DOWNLOADING LIBRARIES AND DEPENDENCIES: \n",
        "from fastbook import *                              # Getting all the Libraries. \n",
        "from fastai.callback.fp16 import *\n",
        "from fastai.text.all import *                       # Getting all the Libraries.\n",
        "from IPython.display import display, HTML"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAY-kcjCd-Tz"
      },
      "source": [
        "### **GETTING THE DATASET:**\n",
        "- I will get the **IMDB Dataset** here. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "iv-s-9D1dsia",
        "outputId": "7972c025-a7a7-4434-80e1-dc1b3c73235f"
      },
      "source": [
        "#@ GETTING THE DATASET: \n",
        "path = untar_data(URLs.IMDB)                       # Getting Path to the Dataset. \n",
        "path.ls()                                          # Inspecting the Path. "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#7) [Path('/root/.fastai/data/imdb/tmp_lm'),Path('/root/.fastai/data/imdb/train'),Path('/root/.fastai/data/imdb/tmp_clas'),Path('/root/.fastai/data/imdb/README'),Path('/root/.fastai/data/imdb/unsup'),Path('/root/.fastai/data/imdb/imdb.vocab'),Path('/root/.fastai/data/imdb/test')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dBavrB_6e_Of",
        "outputId": "4cecdd03-df5d-4140-e1fe-b01ced3c89e2"
      },
      "source": [
        "#@ GETTING TEXT FILES: \n",
        "files = get_text_files(path, folders=[\"train\", \"test\", \"unsup\"])        # Getting Text Files. \n",
        "txt = files[0].open().read()                                            # Getting a Text. \n",
        "txt[:75]                                                                # Inspecting Text. "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"i don't know what they were thinking.by they,i mean anybody even remotely c\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG832kuRgXeo"
      },
      "source": [
        "### **WORD TOKENIZATION:**\n",
        "- **Word Tokenization** splits a sentence on spaces as well as applying language specific rules to try to separate parts of meaning even when there are no spaces. Generally punctuation marks are also split into separate tokens. **Token** is a element of a list created by the **Tokenization** process which could be a word, a part of a word or subword or a single character. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yx1lsBcYgQ9K",
        "outputId": "43cb0156-2b83-4a2f-8592-42350c1b61e6"
      },
      "source": [
        "#@ INITIALIZING WORD TOKENIZATION: \n",
        "spacy = WordTokenizer()                                  # Initializing Tokenizer. \n",
        "toks = first(spacy([txt]))                               # Getting Tokens of Words. \n",
        "print(coll_repr(toks, 30))                               # Inspecting Tokens. "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(#172) ['i','do',\"n't\",'know','what','they','were','thinking.by','they',',','i','mean','anybody','even','remotely','connected','to','this',\"disaster.i've\",'seen','so','bad','movies',',',\"i've\",'seen','so','really','bad','movies'...]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ti9Hi_uriKxe",
        "outputId": "f71441c8-be96-48ae-f64f-29bf5257d58f"
      },
      "source": [
        "#@ INSPECTING TOKENIZATION: EXAMPLE:\n",
        "first(spacy(['The U.S. dollar $1 is $1.00.']))           # Inspecting Tokens. "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#9) ['The','U.S.','dollar','$','1','is','$','1.00','.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liWRboFJi84p",
        "outputId": "5528a7d5-9237-4733-fb07-aa0e42299180"
      },
      "source": [
        "#@ INITIALIZING WORD TOKENIZATION WITH FASTAI: \n",
        "tkn = Tokenizer(spacy)                                   # Initializing Tokenizer. \n",
        "print(coll_repr(tkn(txt), 31))                           # Inspecting Tokens. "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(#175) ['xxbos','i','do',\"n't\",'know','what','they','were','thinking.by','they',',','i','mean','anybody','even','remotely','connected','to','this',\"disaster.i've\",'seen','so','bad','movies',',',\"i've\",'seen','so','really','bad','movies'...]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsVt5tA8P1hU"
      },
      "source": [
        "**Note:**\n",
        "- **xxbos** : Indicates the beginning of a text. \n",
        "- **xxmaj** : Indicates the next word begins with a capital. \n",
        "- **xxunk** : Indicates the next word is unknown.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "X-GgI67Ajr-K",
        "outputId": "947ed884-058f-4bfe-a40e-2f2bbd933399"
      },
      "source": [
        "#@ INSPECTING TOKENIZATION: EXAMPLE:\n",
        "coll_repr(tkn('&copy; Fast.ai www.fast.ai/INDEX'), 30)   # Inspecting Tokens. "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"(#11) ['xxbos','©','xxmaj','fast.ai','xxrep','3','w','.fast.ai','/','xxup','index']\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8clYuvALmBg"
      },
      "source": [
        "### **SUBWORD TOKENIZATION:**\n",
        "- **Word Tokenization** relies on an assumption that spaces provide a useful separation of components of meaning in a sentence which is not always appropriate. Languages such as Chinese and Japanese don't use spaces and in such cases **Subword Tokenization** generally plays the best role. **Subword Tokenization** splits words into smaller parts based on the most commonly occurring sub strings. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO-mr3aLm2QB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "34a1fbd6-44e4-4255-eca2-0384dbbaf8e7"
      },
      "source": [
        "#@ INITIALIZING SUBWORD TOKENIZATION: EXAMPLE:\n",
        "txts = L(o.open().read() for o in files[:2000])                # Getting List of Reviews. \n",
        "\n",
        "#@ INITIALIZING SUBWORD TOKENIZER: \n",
        "def subword(sz):                                               # Defining Function.      \n",
        "    sp = SubwordTokenizer(vocab_sz=sz)                         # Initializing Subword Tokenizer. \n",
        "    sp.setup(txts)                                             # Getting Sequence of Characters. \n",
        "    return \" \".join(first(sp([txt]))[:40])                     # Inspecting the Vocab. \n",
        "\n",
        "#@ IMPLEMENTATION: \n",
        "subword(1000)                                                  # Inspecting Subword Tokenization. "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"▁i ▁don ' t ▁know ▁what ▁they ▁were ▁think ing . b y ▁they , i ▁mean ▁any bo dy ▁even ▁re mo te ly ▁con n ect ed ▁to ▁this ▁dis a ster . i ' ve ▁seen ▁so\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWSi_eQzQcxL"
      },
      "source": [
        "**Notes:**\n",
        "- Here **setup** is a special fastai method that is called automatically in usual data processing pipelines which reads the documents and find the common sequences of characters to create the vocab. Similarly [**L**](https://fastcore.fast.ai/#L) is also referred as superpowered list. The special character '_' represents a space character in the original text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "MpSPYDjIQbVP",
        "outputId": "3838eea0-13e7-4fd2-aaf7-6150cea5474b"
      },
      "source": [
        "#@ IMPLEMENTATION OF SUBWORD TOKENIZATION: \n",
        "subword(200)                                                  # Inspecting Vocab. \n",
        "subword(10000)                                                # Inspecting Vocab. "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"▁i ▁don ' t ▁know ▁what ▁they ▁were ▁thinking . by ▁they , i ▁mean ▁anybody ▁even ▁remote ly ▁connect ed ▁to ▁this ▁disaster . i ' ve ▁seen ▁so ▁bad ▁movies , i ' ve ▁seen ▁so ▁really ▁bad\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTxPsa_RT-oU"
      },
      "source": [
        "**Note:**\n",
        "- A larger vocab means fewer tokens per sentence which means faster training, less memory, and less state for the model to remember but it means larger embedding matrices and require more data to learn. **Subword Tokenization** provides a way to easily scale between character tokenization i.e. using a small subword vocab and word tokenization i.e using a large subword vocab and handles every human language without needing language specific algorithms to be developed. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9scF0s-DVQjf"
      },
      "source": [
        "### **NUMERICALIZATION:**\n",
        "- **Numericalization** is the process of mapping tokens to integers. It involves making a list of all possible levels of that categorical variable or the vocab and replacing each level with its index in the vocab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vV8wXsyFTb42",
        "outputId": "886a455b-5af8-41f9-c95e-54ae6941ab60"
      },
      "source": [
        "#@ INITIALIZING TOKENS: \n",
        "toks = tkn(txt)                                              # Getting Tokens. \n",
        "print(coll_repr(tkn(txt), 31))                               # Inspecting Tokens. "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(#175) ['xxbos','i','do',\"n't\",'know','what','they','were','thinking.by','they',',','i','mean','anybody','even','remotely','connected','to','this',\"disaster.i've\",'seen','so','bad','movies',',',\"i've\",'seen','so','really','bad','movies'...]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GORafHQXscv",
        "outputId": "55585d37-be95-43df-ca83-f8ffeacbaf95"
      },
      "source": [
        "#@ INITIALIZING TOKENS: \n",
        "toks200 = txts[:200].map(tkn)                                # Getting Tokens. \n",
        "toks200[0]                                                   # Inspecting Tokens. "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#175) ['xxbos','i','do',\"n't\",'know','what','they','were','thinking.by','they'...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ta9px3-xZfNs",
        "outputId": "da135ad5-1ec4-4e6e-fea1-943a90361843"
      },
      "source": [
        "#@ NUMERICALIZATION USING FASTAI: \n",
        "num = Numericalize()                                         # Initializing Numericalization. \n",
        "num.setup(toks200)                                           # Getting Integers. \n",
        "coll_repr(num.vocab, 20)                                     # Inspecting Vocabulary. "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"(#2112) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the','.',',','a','of','and','to','is','i','in','it'...]\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tt8fU-YNfROJ",
        "outputId": "ed53825a-198a-4e60-c803-593d41af75f9"
      },
      "source": [
        "#@ INITIALIZING NUMERICALIZATION: \n",
        "nums = num(toks)[:20]; nums                                  # Inspection. \n",
        "\" \".join(num.vocab[o] for o in nums)                         # Getting Original Text. "
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"xxbos i do n't know what they were xxunk they , i mean anybody even remotely xxunk to this xxunk\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uhElEnXf_qY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}