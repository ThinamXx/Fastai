{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk5SHfqIc6Ey"
      },
      "source": [
        "### **INITIALIZATION:**\n",
        "- I use these three lines of code on top of my each notebooks because it will help to prevent any problems while reloading the same project. And the third line of code helps to make visualization within the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9K2einKadKr"
      },
      "source": [
        "#@ INITIALIZATION: \n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTFZaPENdHTs"
      },
      "source": [
        "**LIBRARIES AND DEPENDENCIES:**\n",
        "- I have downloaded all the libraries and dependencies required for the project in one particular cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZDkvIrydDtY"
      },
      "source": [
        "#@ INSTALLING DEPENDENCIES: UNCOMMENT BELOW: \n",
        "# !pip install -Uqq fastbook\n",
        "# import fastbook\n",
        "# fastbook.setup_book()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTIdUUtEdOah"
      },
      "source": [
        "#@ DOWNLOADING LIBRARIES AND DEPENDENCIES: \n",
        "from fastbook import *                              # Getting all the Libraries. \n",
        "from fastai.callback.fp16 import *\n",
        "from fastai.text.all import *                       # Getting all the Libraries.\n",
        "from IPython.display import display, HTML"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAY-kcjCd-Tz"
      },
      "source": [
        "### **GETTING THE DATASET:**\n",
        "- I will get the **IMDB Dataset** here. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "iv-s-9D1dsia",
        "outputId": "e45ff349-4494-4c2d-a21a-6184d08452b7"
      },
      "source": [
        "#@ GETTING THE DATASET: \n",
        "path = untar_data(URLs.IMDB)                       # Getting Path to the Dataset. \n",
        "path.ls()                                          # Inspecting the Path. "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#7) [Path('/root/.fastai/data/imdb/unsup'),Path('/root/.fastai/data/imdb/tmp_clas'),Path('/root/.fastai/data/imdb/train'),Path('/root/.fastai/data/imdb/imdb.vocab'),Path('/root/.fastai/data/imdb/test'),Path('/root/.fastai/data/imdb/README'),Path('/root/.fastai/data/imdb/tmp_lm')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dBavrB_6e_Of",
        "outputId": "87fe6ba0-d4e2-4921-8f5c-09d3aa9d3877"
      },
      "source": [
        "#@ GETTING TEXT FILES: \n",
        "files = get_text_files(path, folders=[\"train\", \"test\", \"unsup\"])        # Getting Text Files. \n",
        "txt = files[0].open().read()                                            # Getting a Text. \n",
        "txt[:75]                                                                # Inspecting Text. "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Well it wasn't horrible but it wasn't great. No where near as good as the o\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG832kuRgXeo"
      },
      "source": [
        "### **WORD TOKENIZATION:**\n",
        "- **Word Tokenization** splits a sentence on spaces as well as applying language specific rules to try to separate parts of meaning even when there are no spaces. Generally punctuation marks are also split into separate tokens. **Token** is a element of a list created by the **Tokenization** process which could be a word, a part of a word or subword or a single character. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yx1lsBcYgQ9K",
        "outputId": "a97b8cdd-70b7-473b-e322-aefacf89acfc"
      },
      "source": [
        "#@ INITIALIZING WORD TOKENIZATION: \n",
        "spacy = WordTokenizer()                                  # Initializing Tokenizer. \n",
        "toks = first(spacy([txt]))                               # Getting Tokens of Words. \n",
        "print(coll_repr(toks, 30))                               # Inspecting Tokens. "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(#298) ['Well','it','was',\"n't\",'horrible','but','it','was',\"n't\",'great','.','No','where','near','as','good','as','the','original','.','It','kinda','tried','to','hard','to','be','the','first','movie'...]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ti9Hi_uriKxe",
        "outputId": "3ab387c8-7c32-4966-bed7-fd8a701d8f3e"
      },
      "source": [
        "#@ INSPECTING TOKENIZATION: EXAMPLE:\n",
        "first(spacy(['The U.S. dollar $1 is $1.00.']))           # Inspecting Tokens. "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#9) ['The','U.S.','dollar','$','1','is','$','1.00','.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liWRboFJi84p",
        "outputId": "dee03077-aa0c-4f98-9f81-44bf1904ebee"
      },
      "source": [
        "#@ INITIALIZING WORD TOKENIZATION WITH FASTAI: \n",
        "tkn = Tokenizer(spacy)                                   # Initializing Tokenizer. \n",
        "print(coll_repr(tkn(txt), 31))                           # Inspecting Tokens. "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(#331) ['xxbos','xxmaj','well','it','was',\"n't\",'horrible','but','it','was',\"n't\",'great','.','xxmaj','no','where','near','as','good','as','the','original','.','xxmaj','it','kinda','tried','to','hard','to','be'...]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsVt5tA8P1hU"
      },
      "source": [
        "**Note:**\n",
        "- **xxbos** : Indicates the beginning of a text. \n",
        "- **xxmaj** : Indicates the next word begins with a capital. \n",
        "- **xxunk** : Indicates the next word is unknown.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "X-GgI67Ajr-K",
        "outputId": "d74da75e-bd5e-4308-c48f-4e2353f7c647"
      },
      "source": [
        "#@ INSPECTING TOKENIZATION: EXAMPLE:\n",
        "coll_repr(tkn('&copy; Fast.ai www.fast.ai/INDEX'), 30)   # Inspecting Tokens. "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"(#11) ['xxbos','Â©','xxmaj','fast.ai','xxrep','3','w','.fast.ai','/','xxup','index']\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8clYuvALmBg"
      },
      "source": [
        "### **SUBWORD TOKENIZATION:**\n",
        "- **Word Tokenization** relies on an assumption that spaces provide a useful separation of components of meaning in a sentence which is not always appropriate. Languages such as Chinese and Japanese don't use spaces and in such cases **Subword Tokenization** generally plays the best role. **Subword Tokenization** splits words into smaller parts based on the most commonly occurring sub strings. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO-mr3aLm2QB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3bc61f99-1e67-4d46-a21e-cbc7c46f0933"
      },
      "source": [
        "#@ INITIALIZING SUBWORD TOKENIZATION: EXAMPLE:\n",
        "txts = L(o.open().read() for o in files[:2000])                # Getting List of Reviews. \n",
        "\n",
        "#@ INITIALIZING SUBWORD TOKENIZER: \n",
        "def subword(sz):                                               # Defining Function.      \n",
        "    sp = SubwordTokenizer(vocab_sz=sz)                         # Initializing Subword Tokenizer. \n",
        "    sp.setup(txts)                                             # Getting Sequence of Characters. \n",
        "    return \" \".join(first(sp([txt]))[:40])                     # Inspecting the Vocab. \n",
        "\n",
        "#@ IMPLEMENTATION: \n",
        "subword(1000)                                                  # Inspecting Subword Tokenization. "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"âWe ll âit âwas n ' t âhorrible âbut âit âwas n ' t âgreat . âNo âwhere ânear âas âgood âas âthe âoriginal . âIt âkind a âtri ed âto âhard âto âbe âthe âfirst âmovie . âI âthink\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWSi_eQzQcxL"
      },
      "source": [
        "**Notes:**\n",
        "- Here **setup** is a special fastai method that is called automatically in usual data processing pipelines which reads the documents and find the common sequences of characters to create the vocab. Similarly [**L**](https://fastcore.fast.ai/#L) is also referred as superpowered list. The special character '_' represents a space character in the original text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "MpSPYDjIQbVP",
        "outputId": "75fbfe62-3b0e-48a7-d503-67e2a52b6c82"
      },
      "source": [
        "#@ IMPLEMENTATION OF SUBWORD TOKENIZATION: \n",
        "subword(200)                                                  # Inspecting Vocab. \n",
        "subword(10000)                                                # Inspecting Vocab. "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"âWell âit âwasn ' t âhorrible âbut âit âwasn ' t âgreat . âNo âwhere ânear âas âgood âas âthe âoriginal . âIt âkind a âtried âto âhard âto âbe âthe âfirst âmovie . âI âthink âit âneeded âa âbetter\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTxPsa_RT-oU"
      },
      "source": [
        "**Note:**\n",
        "- A larger vocab means fewer tokens per sentence which means faster training, less memory, and less state for the model to remember but it means larger embedding matrices and require more data to learn. **Subword Tokenization** provides a way to easily scale between character tokenization i.e. using a small subword vocab and word tokenization i.e using a large subword vocab and handles every human language without needing language specific algorithms to be developed. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9scF0s-DVQjf"
      },
      "source": [
        "### **NUMERICALIZATION:**\n",
        "- **Numericalization** is the process of mapping tokens to integers. It involves making a list of all possible levels of that categorical variable or the vocab and replacing each level with its index in the vocab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vV8wXsyFTb42",
        "outputId": "256b3df0-a2aa-4826-ba1a-74454dfd901d"
      },
      "source": [
        "#@ INITIALIZING TOKENS: \n",
        "toks = tkn(txt)                                              # Getting Tokens. \n",
        "print(coll_repr(tkn(txt), 31))                               # Inspecting Tokens. "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(#331) ['xxbos','xxmaj','well','it','was',\"n't\",'horrible','but','it','was',\"n't\",'great','.','xxmaj','no','where','near','as','good','as','the','original','.','xxmaj','it','kinda','tried','to','hard','to','be'...]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GORafHQXscv",
        "outputId": "47f5ece8-fad8-4402-8181-a6a8e763a528"
      },
      "source": [
        "#@ INITIALIZING TOKENS: \n",
        "toks200 = txts[:200].map(tkn)                                # Getting Tokens. \n",
        "toks200[0]                                                   # Inspecting Tokens. "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#331) ['xxbos','xxmaj','well','it','was',\"n't\",'horrible','but','it','was'...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ta9px3-xZfNs",
        "outputId": "b1dc6542-a1a8-4591-a774-54743f09096b"
      },
      "source": [
        "#@ NUMERICALIZATION USING FASTAI: \n",
        "num = Numericalize()                                         # Initializing Numericalization. \n",
        "num.setup(toks200)                                           # Getting Integers. \n",
        "coll_repr(num.vocab, 20)                                     # Inspecting Vocabulary. "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"(#2120) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the',',','.','and','a','of','to','is','in','it','i'...]\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tt8fU-YNfROJ",
        "outputId": "ac4ee7a6-e7aa-4c61-f910-7c939dd7bcdb"
      },
      "source": [
        "#@ INITIALIZING NUMERICALIZATION: \n",
        "nums = num(toks)[:20]; nums                                  # Inspection. \n",
        "\" \".join(num.vocab[o] for o in nums)                         # Getting Original Text. "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"xxbos xxmaj well it was n't horrible but it was n't great . xxmaj no where near as good as\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_ZJYk8jVowc"
      },
      "source": [
        "### **CREATING BATCHES FOR LANGUAGE MODEL:**\n",
        "- At every epoch I will shuffle the collection of documents and concatenate them into a stream of tokens and cut that stream into a batch of fixedsize consecutive ministreams. The model will then read the ministreams in order. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thsG0lr0Xq1w",
        "outputId": "56362e99-1d77-4877-eee9-043d2ae0c806"
      },
      "source": [
        "#@ CREATING BATCHES FOR LANGUAGE MODEL: \n",
        "nums200 = toks200.map(num)                                   # Initializing Numericalization. \n",
        "dl = LMDataLoader(nums200)                                   # Creating Language Model Data Loaders. \n",
        "\n",
        "#@ INSPECTING FIRST BATCH: \n",
        "x, y = first(dl)                                             # Getting First Batch of Data. \n",
        "x.shape, y.shape                                             # Inspecting Shape of Data. "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 72]), torch.Size([64, 72]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nYkjwoIMZEIQ",
        "outputId": "73ab7655-5530-4077-c6ee-8f8e1d4be81b"
      },
      "source": [
        "#@ INSPECTING THE DATA: \n",
        "\" \".join(num.vocab[o] for o in x[0][:20])                    # Inspecting Independent Variable. "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"xxbos xxmaj well it was n't horrible but it was n't great . xxmaj no where near as good as\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jnoIOJ2oZsNJ",
        "outputId": "179fe7c0-6cda-491d-8d4f-0d9c8c1cc08d"
      },
      "source": [
        "#@ INSPECTING THE DATA: \n",
        "\" \".join(num.vocab[o] for o in y[0][:20])                    # Inspecting Dependent Variable. "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"xxmaj well it was n't horrible but it was n't great . xxmaj no where near as good as the\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYAR0Mx9a9DY"
      },
      "source": [
        "### **TRAINING A TEXT CLASSIFIER:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDYi_GrzfvqP"
      },
      "source": [
        "**LANGUAGE MODEL USING DATABLOCK:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "NjRlvq8KbbXF",
        "outputId": "5b9453c8-24e0-4fa8-f199-4d9ed3deb96a"
      },
      "source": [
        "#@ CREATING LANGUAGE MODEL USING DATABLOCK: \n",
        "get_imdb = partial(get_text_files, folders=[\"train\", \"test\", \"unsup\"])    # Getting Text Files. \n",
        "db = DataBlock(blocks=TextBlock.from_folder(path, is_lm=True),            # Initializing TextBlock. \n",
        "               get_items=get_imdb, splitter=RandomSplitter(0.1))          # Initializing DataBlock. \n",
        "\n",
        "#@ CREATING LANGUAGE MODEL DATALOADERS: \n",
        "dls_lm = db.dataloaders(path, path=path, bs=128, seq_len=80)              # Initializing Data Loaders. "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "t8zab_Z5fBgA",
        "outputId": "a75a06d5-5dde-4af7-ace8-c33647941da3"
      },
      "source": [
        "#@ INSPECTING THE BATCHES OF DATA: \n",
        "dls_lm.show_batch(max_n=2)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>text_</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>xxbos xxmaj this is basically a goofball comedy , with somewhat odd pacing due to some dramatic elements . xxmaj for xxmaj michael xxup j. xxmaj fox and xxmaj paul xxmaj reubens , it was their first xxunk had previously been in a short lived xxup tv series and a xxup tv movie ) . \\n\\n xxmaj since the movie is basically a race / scavenger hunt type movie , like \" cannonball xxmaj run \" , \" it 's</td>\n",
              "      <td>xxmaj this is basically a goofball comedy , with somewhat odd pacing due to some dramatic elements . xxmaj for xxmaj michael xxup j. xxmaj fox and xxmaj paul xxmaj reubens , it was their first xxunk had previously been in a short lived xxup tv series and a xxup tv movie ) . \\n\\n xxmaj since the movie is basically a race / scavenger hunt type movie , like \" cannonball xxmaj run \" , \" it 's a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>concerned only with the behavior of its characters , it 's original and challenging . xxmaj then it turns into a story filled with familiar elements , and by the end everything is happening by the numbers . xxbos xxmaj what goes through the mind of the office drone that snaps and shoots up the place ? xxmaj what drives a person like that ? xxmaj come on , we 've all seen the reports \" he xxmaj was a</td>\n",
              "      <td>only with the behavior of its characters , it 's original and challenging . xxmaj then it turns into a story filled with familiar elements , and by the end everything is happening by the numbers . xxbos xxmaj what goes through the mind of the office drone that snaps and shoots up the place ? xxmaj what drives a person like that ? xxmaj come on , we 've all seen the reports \" he xxmaj was a xxmaj</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oa7FZgFegs1r"
      },
      "source": [
        "**FINETUNING THE LANGAUGE MODEL:**\n",
        "- I will use **Embeddings** to convert the integer word indices into activations that can be used for the neural networks. These embeddings are feed into **Recurrent Neural Network** using and architecture called **AWD-LSTM**. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "K3Qda2V4f9O_",
        "outputId": "a47f5c8e-cdbc-4e47-c295-a0186abad3e4"
      },
      "source": [
        "#@ INITIALIZING LANGUAGE MODEL LEARNER: \n",
        "learn = language_model_learner(dls_lm, AWD_LSTM, drop_mult=0.3,                 # Using AWD LSTM Architecture. \n",
        "                               metrics=[accuracy, Perplexity()]).to_fp16()      # Initializing LM Learner.    "
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "DwMl0Jl5mZuA",
        "outputId": "859de7e5-7d5f-4dff-cf34-dcc1890894bb"
      },
      "source": [
        "#@ TRAINING EMBEDDINGS WITH RANDOM INITIALIZATION: \n",
        "learn.fit_one_cycle(1, 2e-2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>perplexity</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>4.006634</td>\n",
              "      <td>3.903991</td>\n",
              "      <td>0.299049</td>\n",
              "      <td>49.600018</td>\n",
              "      <td>21:03</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlBstN_DoYVY"
      },
      "source": [
        "**SAVING AND LOADING MODELS:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJP-rFdEnDH_",
        "outputId": "e85809a6-da32-49e1-82e8-2b4eec8aaa7e"
      },
      "source": [
        "#@ SAVING MODELS: \n",
        "learn.save(\"/content/gdrive/MyDrive/1Epoch\")                  # Saving the Model. "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Path('/content/gdrive/MyDrive/1Epoch.pth')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UweY10WKotCV"
      },
      "source": [
        "#@ LOADING MODELS: \n",
        "learn = learn.load(\"/content/gdrive/MyDrive/1Epoch\")          # Loading the Model. "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "cqgqbPgQxn7o",
        "outputId": "707075b6-6ad5-40fb-e3cf-a4c1d1ef1887"
      },
      "source": [
        "#@ FINETUNING THE LANGUAGE MODEL: \n",
        "learn.unfreeze()                                              # Unfreezing the Layers. \n",
        "learn.fit_one_cycle(10, 2e-3)                                 # Training the Model. "
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>perplexity</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>4.338380</td>\n",
              "      <td>4.253496</td>\n",
              "      <td>0.291754</td>\n",
              "      <td>70.350945</td>\n",
              "      <td>22:53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.975756</td>\n",
              "      <td>3.908851</td>\n",
              "      <td>0.313350</td>\n",
              "      <td>49.841671</td>\n",
              "      <td>23:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.798644</td>\n",
              "      <td>3.762220</td>\n",
              "      <td>0.323882</td>\n",
              "      <td>43.043861</td>\n",
              "      <td>22:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.690822</td>\n",
              "      <td>3.682423</td>\n",
              "      <td>0.330617</td>\n",
              "      <td>39.742588</td>\n",
              "      <td>22:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.606129</td>\n",
              "      <td>3.640616</td>\n",
              "      <td>0.334283</td>\n",
              "      <td>38.115314</td>\n",
              "      <td>22:53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.529613</td>\n",
              "      <td>3.610161</td>\n",
              "      <td>0.337432</td>\n",
              "      <td>36.972008</td>\n",
              "      <td>23:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.455461</td>\n",
              "      <td>3.589717</td>\n",
              "      <td>0.339728</td>\n",
              "      <td>36.223839</td>\n",
              "      <td>23:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>3.384331</td>\n",
              "      <td>3.582524</td>\n",
              "      <td>0.341028</td>\n",
              "      <td>35.964203</td>\n",
              "      <td>23:23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>3.340192</td>\n",
              "      <td>3.580934</td>\n",
              "      <td>0.341526</td>\n",
              "      <td>35.907047</td>\n",
              "      <td>23:15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>3.312969</td>\n",
              "      <td>3.583758</td>\n",
              "      <td>0.341377</td>\n",
              "      <td>36.008621</td>\n",
              "      <td>23:11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKYtCD3ozJ8f"
      },
      "source": [
        "**ENCODER**\n",
        "- **Encoder** is defined as the model which doesn't contain task specific final layers. The term **Encoder** means much the same thing as body when applied to vision **CNN** but **Encoder** tends to be more used for NLP and generative models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmDWEDIfy3Pw"
      },
      "source": [
        "#@ SAVING MODELS: \n",
        "learn.save_encoder(\"/content/gdrive/MyDrive/FineTuned\")       # Saving the Model. "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmrreawCleIT"
      },
      "source": [
        "### **TEXT GENERATION:**\n",
        "- I will use the model to write new reviews. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0qs7j0P1Hio",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "c17e773c-5e03-4745-81b6-148b7cbb66ae"
      },
      "source": [
        "#@ INITIALIZING TEXT GENERATION: \n",
        "TEXT = \"I hate the movie because\"                            # Text Example. \n",
        "N_WORDS = 40\n",
        "N_SENTENCES = 2\n",
        "preds = [learn.predict(TEXT, N_WORDS, temperature=0.75)\n",
        "         for _ in range(N_SENTENCES)]                       # Initializing Text Generation. \n",
        "print(\" \".join(preds))                                      # Inspecting Generated Reviews.  "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "i hate the movie because of the awful acting and the Canadian scenery . \n",
            "\n",
            " This movie is not worth a penny . There is nothing funny or funny about this movie . Here 's another one of those b movies i hate the movie because it is so predictable and ridiculous . No , this one is not for people who are not . This movie is a \" thriller \" , with a lot of nudity and problem in the plot .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wtpZHuJn966"
      },
      "source": [
        "### **TEXT CLASSIFICATION:**\n",
        "- Here, I am moving towards **Classifier** fine tuning rather than **Language Model** fine tuning as mentioned above. A **Language Model** predicts the next word of a document so it doesn't require any external labels. A **Classifier** predicts an external labels. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "xpjJuTaUo65Z",
        "outputId": "87c20cf4-06fb-4585-bd7b-31ef88eeefea"
      },
      "source": [
        "#@ INITIALIZING DATALOADERS:\n",
        "db_clas = DataBlock(blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab),        # Initializing Text Blocks. \n",
        "                            CategoryBlock),                                         # Initializing Category Block.\n",
        "                    get_y=parent_label,                                             # Getting Target. \n",
        "                    get_items=partial(get_text_files,folders=[\"train\",\"test\"]),     # Getting Text Files. \n",
        "                    splitter=GrandparentSplitter(valid_name=\"test\"))                # Splitting the Data. \n",
        "dls_clas = db_clas.dataloaders(path, path=path, bs=128, seq_len=72)                 # Initializing DataLoaders. \n",
        "\n",
        "#@ INSPECTING THE BATCHES: \n",
        "dls_clas.show_batch(max_n=2)                                                        # Inspection. "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>xxbos xxmaj match 1 : xxmaj tag xxmaj team xxmaj table xxmaj match xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley vs xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley started things off with a xxmaj tag xxmaj team xxmaj table xxmaj match against xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit . xxmaj according to the rules of the match , both opponents have to go through tables in order to get the win . xxmaj benoit and xxmaj guerrero heated up early on by taking turns hammering first xxmaj spike and then xxmaj bubba xxmaj ray . a xxmaj german xxunk by xxmaj benoit to xxmaj bubba took the wind out of the xxmaj dudley brother . xxmaj spike tried to help his brother , but the referee restrained him while xxmaj benoit and xxmaj guerrero</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>xxbos * * attention xxmaj spoilers * * \\n\\n xxmaj first of all , let me say that xxmaj rob xxmaj roy is one of the best films of the 90 's . xxmaj it was an amazing achievement for all those involved , especially the acting of xxmaj liam xxmaj neeson , xxmaj jessica xxmaj lange , xxmaj john xxmaj hurt , xxmaj brian xxmaj cox , and xxmaj tim xxmaj roth . xxmaj michael xxmaj canton xxmaj jones painted a wonderful portrait of the honor and dishonor that men can represent in themselves . xxmaj but alas â¦ \\n\\n it constantly , and unfairly gets compared to \" braveheart \" . xxmaj these are two entirely different films , probably only similar in the fact that they are both about xxmaj scots in historical xxmaj scotland . xxmaj yet , this comparison frequently bothers me because it seems</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3zGfwJ119qn"
      },
      "source": [
        "#@ CREATING MODEL FOR TEXT CLASSIFICATION: \n",
        "learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, \n",
        "                                metrics=accuracy).to_fp16()                        # Initializing Text Classifier Learner. \n",
        "learn = learn.load_encoder(\"/content/gdrive/MyDrive/FineTuned\")                    # Loading the Encoder. "
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj-j8-CZ4yK0"
      },
      "source": [
        "**FINETUNING THE CLASSIFIER:**\n",
        "- I will train the **Classifier** with discriminative learning rates and gradaul unfreezing. In computer vision unfreezing the model at once is common approach but for **NLP Classifier** unfreezing a few layers at a time will make a real difference. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "I998L26w2riy",
        "outputId": "a625671a-e120-433f-9b98-645b4ce01644"
      },
      "source": [
        "#@ TRAINING THE CLASSIFIERS: \n",
        "learn.fit_one_cycle(1, 2e-2)                                                       # Freezing. "
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.238200</td>\n",
              "      <td>0.180242</td>\n",
              "      <td>0.932560</td>\n",
              "      <td>01:09</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "eYa6uUIA2tyR",
        "outputId": "ee40e601-2b10-4787-8c3d-ed37812c4175"
      },
      "source": [
        "#@ TRAINING THE CLASSIFIERS: UNFREEZING LAYERS: \n",
        "learn.freeze_to(-2)                                                               # Unfreezing. \n",
        "learn.fit_one_cycle(1, slice(1e-2/(2.6**4), 1e-2))                                # Training the Classifier. "
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.219655</td>\n",
              "      <td>0.163169</td>\n",
              "      <td>0.937880</td>\n",
              "      <td>01:14</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "IDYKJTwf7X5B",
        "outputId": "c8e5159c-4663-4ed7-d752-49fce5473d07"
      },
      "source": [
        "#@ TRAINING THE CLASSIFIERS: UNFREEZING LAYERS: \n",
        "learn.freeze_to(-3)                                                               # Unfreezing. \n",
        "learn.fit_one_cycle(1, slice(5e-3/(2.6**4), 5e-3))                                # Training the Classifier. "
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.187817</td>\n",
              "      <td>0.147782</td>\n",
              "      <td>0.944600</td>\n",
              "      <td>01:37</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "yN2WgnOR7r1P",
        "outputId": "e427cd5b-82e8-4f90-ac4b-55fecc19a505"
      },
      "source": [
        "#@ TRAINING THE CLASSIFIERS: UNFREEZING LAYERS: \n",
        "learn.unfreeze()                                                                  # Unfreezing. \n",
        "learn.fit_one_cycle(2, slice(1e-3/(2.6**4), 1e-3))                                # Training the Classifier. "
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.158834</td>\n",
              "      <td>0.149136</td>\n",
              "      <td>0.944480</td>\n",
              "      <td>01:55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.143565</td>\n",
              "      <td>0.148690</td>\n",
              "      <td>0.947720</td>\n",
              "      <td>01:55</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}